{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjf_eGJ8fjPC"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"hf_creditibility_score.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1qbkjtHQwHOLbvs7kTJKI2SNq186ufDHi\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten\n",
        "\n",
        "# Define function to create the neural network model\n",
        "def create_nn_model(vocab_size: int, embedding_dim: int, max_length: int, num_of_dense: int) -> Model:\n",
        "    \"\"\"\n",
        "    Creates a neural network model that processes user prompts using an embedding layer,\n",
        "    concatenates it with function ratings, and passes through dense layers.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Size of the vocabulary for embedding.\n",
        "        embedding_dim (int): Dimensionality of the embedding layer.\n",
        "        max_length (int): Maximum length of input sequences.\n",
        "        num_of_dense (int): Number of dense layers before concatenation.\n",
        "\n",
        "    Returns:\n",
        "        Model: A compiled TensorFlow model.\n",
        "    \"\"\"\n",
        "    # Text input (user prompt)\n",
        "    text_input = Input(shape=(max_length,), name=\"text_input\")\n",
        "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(text_input)\n",
        "    flatten = Flatten()(embedding)\n",
        "\n",
        "    # Dense layers for text input\n",
        "    num_neurons = 2**12  # Start with 4096 neurons\n",
        "    x = flatten\n",
        "    for _ in range(num_of_dense):\n",
        "        num_neurons = max(1, int(num_neurons / 2))  # Ensure integer neurons, minimum of 1\n",
        "        x = Dense(num_neurons, activation='relu')(x)\n",
        "\n",
        "    # Numeric input (func_rating)\n",
        "    func_rating_input = Input(shape=(1,), name=\"func_rating_input\")\n",
        "    y = Dense(32, activation='relu')(func_rating_input)\n",
        "\n",
        "    # Concatenate both paths\n",
        "    concatenated = Concatenate()([x, y])\n",
        "    # output = Dense(1, activation='linear', name=\"output\")(concatenated)\n",
        "    output = Dense(6, activation='softmax', name=\"output\")(concatenated)\n",
        "\n",
        "    # Define and compile the model\n",
        "    model = Model(inputs=[text_input, func_rating_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "import os\n",
        "print(\"Files in directory:\", os.listdir())\n",
        "\n",
        "# Try reading the saved CSV file\n",
        "df = pd.read_csv(\"combined_data (1).csv\")\n",
        "print(df.head())  # Display first few rows\n",
        "\n",
        "df[\"custom_rating\"].unique()\n",
        "\n",
        "df.shape\n",
        "\n",
        "# Tokenize and prepare data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[\"user_prompt\"])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max([len(x.split()) for x in df[\"user_prompt\"]])\n",
        "embedding_dim = 16\n",
        "\n",
        "# Convert text data into sequences\n",
        "X_text = tokenizer.texts_to_sequences(df[\"user_prompt\"])\n",
        "X_text = pad_sequences(X_text, maxlen=max_length, padding='post')\n",
        "print(X_text.shape)\n",
        "\n",
        "# Numeric input\n",
        "X_func_rating = np.array(df[\"func_rating\"]).reshape(-1, 1)\n",
        "print(X_func_rating.shape)\n",
        "\n",
        "# Target variable\n",
        "y = np.array(df[\"custom_rating\"]).reshape(-1, 1)\n",
        "print(y.shape)\n",
        "\n",
        "df[\"custom_rating\"].unique()\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Assuming y is your array of class labels shaped as (20, 1)\n",
        "# Convert labels to one-hot encoding\n",
        "y_one_hot = to_categorical(y)\n",
        "\n",
        "# Check the new shape of y_one_hot\n",
        "print(y_one_hot.shape)\n",
        "\n",
        "# Create the model with updated Embedding layer\n",
        "num_of_dense_layers = 3  # Number of dense layers before concatenation\n",
        "\n",
        "model = create_nn_model(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    max_length=max_length,\n",
        "    num_of_dense=num_of_dense_layers  # Ensure correct parameter naming\n",
        ")\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %%time\n",
        "#\n",
        "# # Train the model\n",
        "# model.fit(\n",
        "#     {\"text_input\": X_text, \"func_rating_input\": X_func_rating},\n",
        "#     y_one_hot,\n",
        "#     epochs=80,\n",
        "#     batch_size=2,\n",
        "#     validation_split=0.1,\n",
        "#     verbose=2\n",
        "# )\n",
        "\n",
        "# Plot error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(model.history.history['loss'])\n",
        "plt.plot(model.history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"huggingface_hub\"])\n",
        "\n",
        "!huggingface-cli logout  # This will remove all saved tokens\n",
        "\n",
        "!huggingface-cli login\n",
        "\n",
        "import requests\n",
        "\n",
        "HF_TOKEN = \"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\"  # Replace with your actual token\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "response = requests.get(\"https://huggingface.co/api/whoami-v2\", headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\" Token is valid:\", response.json())\n",
        "else:\n",
        "    print(f\" Token is INVALID. Status Code: {response.status_code}, Response: {response.text}\")\n",
        "\n",
        "!huggingface-cli login\n",
        "\n",
        "import requests\n",
        "\n",
        "HF_TOKEN = \"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\"  # Replace with your actual token\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "response = requests.get(\"https://huggingface.co/api/whoami-v2\", headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\" Token is valid:\", response.json())\n",
        "else:\n",
        "    print(f\" Token is INVALID. Status Code: {response.status_code}, Response: {response.text}\")\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Define Hugging Face credentials\n",
        "hf_username = \"KAMAL18\"  # Ensure this matches your exact username\n",
        "repo_name = \"my-tf-nn-model-v18\"\n",
        "repo_id = f\"{hf_username}/{repo_name}\"\n",
        "\n",
        "# Use your valid token (paste your actual token)\n",
        "HF_TOKEN = \"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\"  # Replace with your correct token\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "# Create the repository using explicit authentication\n",
        "api.create_repo(repo_id=repo_id, token=HF_TOKEN, exist_ok=True)\n",
        "\n",
        "print(f\" Repository created successfully: https://huggingface.co/{repo_id}\")\n",
        "\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "import requests\n",
        "\n",
        "HF_TOKEN = \"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\"  # Paste your token\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "response = requests.get(\"https://huggingface.co/api/whoami-v2\", headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\" Token is valid:\", response.json())\n",
        "else:\n",
        "    print(f\" Token is INVALID. Status Code: {response.status_code}, Response: {response.text}\")\n",
        "\n",
        "!huggingface-cli logout  # Log out first\n",
        "!huggingface-cli login   # Log in again\n",
        "\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "import os\n",
        "HF_TOKEN = \"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\"  # Replace with your actual token\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN  # Store token in environment\n",
        "\n",
        "from huggingface_hub import whoami\n",
        "\n",
        "try:\n",
        "    user_info = whoami(token=os.environ[\"HF_TOKEN\"])\n",
        "    print(f\" Authentication successful: {user_info['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\" Authentication failed: {e}\")\n",
        "\n",
        "import os\n",
        "\n",
        "# Set Keras backend to JAX (before importing Keras/TensorFlow)\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# ðŸ”¹ Define Hugging Face repository details\n",
        "repo_id = \"KAMAL18/my-tf-nn-model-v18\"  # Change as needed\n",
        "HF_TOKEN = \"your_actual_huggingface_token_here\"  # Replace with your token\n",
        "\n",
        "# ðŸ”¹ Define the model filename (Ensure it matches the uploaded file)\n",
        "filename = \"model.keras\"\n",
        "\n",
        "# ðŸ”¹ Download the model from Hugging Face\n",
        "try:\n",
        "    model_path = hf_hub_download(repo_id=repo_id, filename=filename, token=HF_TOKEN)\n",
        "    print(f\" Model downloaded successfully: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\" Failed to download model: {e}\")\n",
        "    exit()  # Stop execution if model fails to download\n",
        "\n",
        "# ðŸ”¹ Load the Keras model\n",
        "try:\n",
        "    model = keras.models.load_model(model_path)\n",
        "    print(\" Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\" Error loading model: {e}\")\n",
        "    exit()  # Stop execution if model fails to load\n",
        "\n",
        "model.summary()\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#  Define Hugging Face repository details\n",
        "repo_id = \"KAMAL18/my-tf-nn-model-v18\"  # Change as needed\n",
        "HF_TOKEN = \"your_actual_huggingface_token_here\"  # Replace with your token\n",
        "\n",
        "try:\n",
        "    #  Download and load the model\n",
        "    model_path = hf_hub_download(repo_id=repo_id, filename=\"model.keras\", token=HF_TOKEN)\n",
        "    model = keras.models.load_model(model_path)\n",
        "    print(\" Model loaded successfully!\")\n",
        "\n",
        "    #  Download and load the tokenizer\n",
        "    tokenizer_path = hf_hub_download(repo_id=repo_id, filename=\"tokenizer.pkl\", token=HF_TOKEN)\n",
        "    with open(tokenizer_path, \"rb\") as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    print(\" Tokenizer loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error loading model/tokenizer: {e}\")\n",
        "    exit()\n",
        "\n",
        "#  Sample test data\n",
        "test_texts = [\n",
        "    \"How to improve focus and concentration?\",\n",
        "    \"What are the side effects of lack of sleep?\",\n",
        "]\n",
        "\n",
        "#  Preprocess text input\n",
        "max_length = model.input_shape[1] if isinstance(model.input_shape, tuple) else 10  # Default length if unknown\n",
        "X_text_test = pad_sequences(tokenizer.texts_to_sequences(test_texts), maxlen=max_length, padding='post')\n",
        "\n",
        "#  Automatically detect if the model was trained with text-only or text + numeric inputs\n",
        "expected_input_shape = model.input_shape[1]\n",
        "\n",
        "if expected_input_shape == X_text_test.shape[1]:\n",
        "    X_test_final = X_text_test.astype(np.float32)  # Convert to float32 for TensorFlow compatibility\n",
        "    print(\" Model trained on text only. Using text input.\")\n",
        "\n",
        "elif expected_input_shape == X_text_test.shape[1] + 1:\n",
        "    X_func_test = np.array([[5], [4]], dtype=np.float32)  # Convert to float32\n",
        "    X_test_final = np.hstack((X_text_test, X_func_test))  # Merge inputs\n",
        "    print(\" Model trained on text + numeric input. Merging inputs.\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\" Model expected {expected_input_shape} features, but got {X_text_test.shape[1]} or {X_text_test.shape[1] + 1}.\")\n",
        "\n",
        "#  Make predictions\n",
        "predictions = model.predict(X_test_final)\n",
        "\n",
        "#  Display results in the expected format\n",
        "print(\"\\n **Predictions:**\")\n",
        "for text, pred in zip(test_texts, predictions):\n",
        "    print(f\" Prompt: {text}\")\n",
        "    print(f\" Predicted Rating: {pred[0]:.2f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Load the model\n",
        "model = keras.models.load_model(model_path)\n",
        "\n",
        "#  Recompile the model to reset the optimizer state\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])  # Adjust loss if needed\n",
        "\n",
        "print(\" Model loaded and recompiled successfully!\")\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # This will prompt you to select the file\n",
        "\n",
        "import os\n",
        "\n",
        "print(os.listdir(\"/content\"))  # This should now include tokenizer.pkl\n",
        "\n",
        "import pickle\n",
        "\n",
        "file_path = \"/content/tokenizer (3).pkl\"  # Make sure the name matches your uploaded file\n",
        "\n",
        "# Load the tokenizer object\n",
        "with open(file_path, \"rb\") as file:\n",
        "    tokenizer = pickle.load(file)\n",
        "\n",
        "print(\"Tokenizer loaded successfully!\")\n",
        "\n",
        "# Check the vocabulary (word to index mapping)\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "# Check the reverse mapping (index to word)\n",
        "print(tokenizer.index_word)\n",
        "\n",
        "# Check the number of words in the tokenizer\n",
        "print(len(tokenizer.word_index))\n",
        "\n",
        "# Check word frequency counts\n",
        "print(tokenizer.word_counts)\n",
        "\n",
        "import json\n",
        "\n",
        "# Convert tokenizer to JSON format\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "\n",
        "# Save JSON file\n",
        "with open(\"tokenizer.json\", \"w\") as json_file:\n",
        "    json.dump(tokenizer_json, json_file, indent=4)\n",
        "\n",
        "print(\"Tokenizer saved as JSON. You can now open and inspect it!\")\n",
        "\n",
        "import json\n",
        "\n",
        "# Convert tokenizer to JSON and pretty-print it\n",
        "print(json.dumps(json.loads(tokenizer.to_json()), indent=4))\n",
        "\n"
      ]
    }
  ]
}